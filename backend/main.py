"""
AI News Management System - Main FastAPI Application
"""
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
import asyncio
import time
import sys
import threading
from loguru import logger

# Configure loguru to output to stdout
logger.remove()
logger.add(sys.stdout, level="INFO")

from app.core.config import settings
from app.core.database import connect_to_mongo, close_mongo_connection, get_database
from app.api.v1 import api_router
from apscheduler.schedulers.background import BackgroundScheduler  # Thay ƒë·ªïi: D√πng BackgroundScheduler
from app.services.rabbitmq_service import rabbitmq_service
from app.services.generative_newspaper import generative_newspaper
from datetime import datetime, timezone

# Kh·ªüi t·∫°o scheduler - CH·∫†Y TRONG THREAD RI√äNG
scheduler = BackgroundScheduler(daemon=True)  # daemon=True ƒë·ªÉ t·ª± ƒë·ªông t·∫Øt khi app shutdown

# ===== KEYWORD QUEUES =====
# List A: Keywords ƒë√£ c√≥ (ƒë·ªÉ tr√°nh tr√πng)
existing_keywords = set()
# Queue B: Keywords ch·ªù t·∫°o content
pending_keywords = []

# Kh·ªüi t·∫°o generative newspaper
gen_news = None

def fetch_keywords_job():
    """L·ªãch 1: M·ªói 10 ph√∫t l·∫•y keywords m·ªõi - CH·∫†Y TRONG THREAD RI√äNG"""
    global existing_keywords, pending_keywords, gen_news
    try:
        logger.info("üîç Fetching trending keywords...")
        
        # L·∫•y keywords t·ª´ API ho·∫∑c fallback
        keywords = gen_news.get_trending_keywords()
        logger.info(f"üìä Got {len(keywords)} keywords")
        
        # Ki·ªÉm tra v√† th√™m keywords m·ªõi
        new_count = 0
        for keyword in keywords:
            if keyword not in existing_keywords:
                existing_keywords.add(keyword)
                pending_keywords.append(keyword)
                new_count += 1
                logger.info(f"‚ûï New keyword: {keyword}")
        
        logger.info(f"‚úÖ Added {new_count} new keywords. Queue size: {len(pending_keywords)}")
        
    except Exception as e:
        logger.error(f"‚ùå Fetch keywords error: {e}")

def generate_article_job():
    """L·ªãch 2: M·ªói 30 gi√¢y t·∫°o 1 article t·ª´ queue - CH·∫†Y TRONG THREAD RI√äNG"""
    global pending_keywords, gen_news
    try:
        if not pending_keywords:
            logger.info("‚è∏Ô∏è No keywords in queue")
            return
        
        # L·∫•y keyword ƒë·∫ßu ti√™n
        keyword = pending_keywords.pop(0)
        logger.info(f"üìù Generating article for: {keyword}")
        
        # Sinh n·ªôi dung article b·∫±ng AI (SYNC - kh√¥ng block main thread v√¨ ƒëang ·ªü thread ri√™ng)
        logger.info(f"ü§ñ Calling AI to generate content...")
        article_data = gen_news.generate_full_article(keyword)
        
        # T·∫°o document ho√†n ch·ªânh
        now = datetime.utcnow().isoformat() + "Z"
        
        # Import pymongo sync ƒë·ªÉ insert t·ª´ background thread
        from pymongo import MongoClient
        import os
        client = MongoClient(os.getenv("MONGODB_URI"))
        db = client[os.getenv("MONGODB_DB_NAME")]
        
        # ƒê·∫øm articles hi·ªán t·∫°i
        count = db.articles.count_documents({})
        new_id = count + 1
        
        article = {
            "_id": str(new_id),
            "title": article_data.get("title", ""),
            "slug": article_data.get("slug", ""),
            "excerpt": article_data.get("excerpt", ""),
            "content": article_data.get("content", ""),
            "categoryId": new_id,
            "category": article_data.get("category", "Technology"),
            "authorId": 1,
            "author": "admin",
            "authorAvatar": "https://lh3.googleusercontent.com/a/ACg8ocKW3VsSBWwRkgu3VU4vz0AHItfbhGKlYbgqLXJAihtr-QYgMO1A3g9_eyrAbqOxANa7qc=w240-h480-rw",
            "status": "published",
            "featured": True,
            "views": 100,
            "likes": 100,
            "commentsCount": 100,
            "readTime": "1 min",
            "tags": article_data.get("tags", [keyword]),
            "thumbnail": article_data.get("thumbnail", ""),
            "publishedAt": now,
            "updatedAt": now,
            "createdAt": now
        }
        
        # Insert v√†o MongoDB
        result = db.articles.insert_one(article)
        client.close()
        
        logger.info(f"‚úÖ Created article: {article['title']} (ID: {new_id})")
        logger.info(f"üì¶ Queue remaining: {len(pending_keywords)}")
        
    except Exception as e:
        logger.error(f"‚ùå Generate article error: {e}")
        import traceback
        traceback.print_exc()

async def scheduled_fetch_job():
    """H√†m async ri√™ng ƒë·ªÉ APScheduler g·ªçi"""
    try:
        if rabbitmq_service.enabled:
            logger.info("Scheduler: Publishing news fetch task...")
            await rabbitmq_service.publish_news_fetch_task("https://cointelegraph.com/rss")
        else:
            logger.info("Scheduler: RabbitMQ disabled, skipping task publish.")
    except Exception as e:
        logger.error(f"Scheduler failed to publish task: {e}")

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Qu·∫£n l√Ω c√°c s·ª± ki·ªán startup v√† shutdown c·ªßa ·ª©ng d·ª•ng.
    """
    
    # === STARTUP (Kh·ªüi ƒë·ªông) ===
    logger.info("üöÄ Starting AI News Management System...")
    
    # 1. K·∫øt n·ªëi MongoDB
    await connect_to_mongo()
    logger.info("‚úÖ Connected to MongoDB Atlas")

    # 2. Kh·ªüi t·∫°o generative newspaper
    global gen_news
    gen_news = generative_newspaper(
        api_key=settings.GEMINI_API_KEY,
        unsplash_api_key=settings.UNSPLASH_ACCESS_KEY
    )
    logger.info("‚úÖ Initialized generative newspaper")

    # 3. K·∫øt n·ªëi RabbitMQ
    await rabbitmq_service.connect()

    # 4. Kh·ªüi ƒë·ªông Schedulers trong THREAD RI√äNG (ho√†n to√†n t√°ch bi·ªát v·ªõi FastAPI main thread)
    logger.info("Starting task schedulers in SEPARATE THREAD...")
    
    # Scheduler 1: Fetch keywords m·ªói 10 ph√∫t - CH·∫†Y NGAY
    scheduler.add_job(
        fetch_keywords_job,
        'interval',
        minutes=5,
        id='fetch_keywords',
        next_run_time=datetime.now(timezone.utc)  # Ch·∫°y ngay l·∫≠p t·ª©c
    )
    logger.info("‚úÖ Scheduler 1: Fetch keywords every 10 mins (starts immediately)")
    
    # Scheduler 2: Generate article m·ªói 30 gi√¢y
    scheduler.add_job(
        generate_article_job,
        'interval',
        seconds=10,
        id='generate_article'
    )
    logger.info("‚úÖ Scheduler 2: Generate article every 30 secs")
    
    # Start scheduler - Ch·∫°y trong background thread ri√™ng
    scheduler.start()
    logger.info("‚úÖ All schedulers started in BACKGROUND THREAD!")
    logger.info("‚è≠Ô∏è Scheduler 1 will fetch keywords immediately, then every 10 mins")

    # === YIELD (·ª®ng d·ª•ng ch·∫°y ·ªü ƒë√¢y) ===
    try:
        yield
    finally:
        # === SHUTDOWN (T·∫Øt ·ª©ng d·ª•ng) ===
        logger.info("üëã Shutting down...")
        
        # 1. T·∫Øt Scheduler
        try:
            if scheduler.running:
                scheduler.shutdown()
                logger.info("‚úÖ Task scheduler shut down")
        except Exception as e:
            logger.error(f"‚ùå Error shutting down scheduler: {e}")
        
        # 2. Ng·∫Øt k·∫øt n·ªëi RabbitMQ
        await rabbitmq_service.close() # <-- D√íNG M·ªöI QUAN TR·ªåNG

        # 3. Ng·∫Øt k·∫øt n·ªëi MongoDB
        await close_mongo_connection()
        logger.info("‚úÖ Disconnected from MongoDB")


app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    description="AI-powered News Content Management & Distribution Platform",
    docs_url="/api/docs",
    redoc_url="/api/redoc",
    lifespan=lifespan  # <-- S·ª≠ d·ª•ng h√†m lifespan ƒë√£ g·ªôp ·ªü tr√™n
)

# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Request logging middleware
@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    process_time = time.time() - start_time
    logger.info(
        f"{request.method} {request.url.path} "
        f"completed in {process_time:.2f}s with status {response.status_code}"
    )
    
    return response


# Health check endpoint
@app.get("/health", tags=["Health"])
async def health_check():
    """Check if the service is running"""
    return {
        "status": "healthy",
        "app": settings.APP_NAME,
        "version": settings.APP_VERSION,
        "environment": "development" if settings.DEBUG else "production"
    }


# Include API routes
app.include_router(api_router, prefix="/api/v1")


# Global exception handler
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"Global exception: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "success": False,
            "error": "Internal server error",
            "message": str(exc) if settings.DEBUG else "An error occurred"
        }
    )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.DEBUG,
        log_level="info"
    )